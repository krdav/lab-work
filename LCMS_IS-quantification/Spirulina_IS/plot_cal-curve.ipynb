{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, re, copy\n",
    "import dill as pickle # this is so convenient, it serializes all the functions inside the quantification dict\n",
    "import numpy as np\n",
    "from scipy.optimize import newton, minimize, fsolve\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.backends.backend_pdf\n",
    "import matplotlib.colors as mcolors\n",
    "palette = list(mcolors.TABLEAU_COLORS.keys())\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\", {\n",
    " 'axes.spines.bottom': True,\n",
    " 'axes.spines.left': True,\n",
    " 'axes.spines.right': True,\n",
    " 'axes.spines.top': True\n",
    "})\n",
    "sns.set(font_scale=1)\n",
    "palette = list(mcolors.TABLEAU_COLORS.keys())\n",
    "sns.set_theme(style=\"ticks\", palette=\"muted\")\n",
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit and plot calibration curve for amino acid quantification\n",
    "This program takes LCMS data from TraceFinder and generates calibration curves to quantify compounds using isotopically labelled internal standards. The input data is a dilution series of compounds with a constant concentration of isotopically internal standard associated for each compound. The peak area of each compound is normalized to its internal standard (this is the \"response ratio\") and a curve is fitted to this data. The curve fitting will choose the best curve among 1) linear fit, 2) power fit and 3) 2. degree polynomial fit. The chosen curve will be used for interpolation and two linear fits will be used for extrapolation; one below the lowest concentration in the calibration curve (forced to intersect 0) and one above the highest concentration in the calibration curve. The three curves are connected to a piecewise fit. The loss function used to chose and fit the curves are defined as the sum of the percentage deviation between true and predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This cell contains all functions related to curve fitting ###\n",
    "\n",
    "# Loss functions:\n",
    "def loss_func_c1(t, y):\n",
    "    return((np.abs(y - t) / t) * 100)\n",
    "def loss_func_l1(t, y):\n",
    "    return(np.abs(y - t))  # l1\n",
    "def loss_func_l2(t, y):\n",
    "    return((y - t)**2)     # l2\n",
    "\n",
    "# Linear fit intersecting 0:\n",
    "def lin_fit(x, beta):\n",
    "    return(x*beta)\n",
    "def obj_lin_fit(loss_func, amt_uM, rsp_ratio, p):\n",
    "    beta = p[0]\n",
    "    y = lin_fit(rsp_ratio, beta)\n",
    "    loss = sum(loss_func(amt_uM, y))\n",
    "    return(loss)\n",
    "# Linear fit allowed to be offset 0:\n",
    "def lin_fit_off(x, alpha, beta):\n",
    "    return(alpha + x*beta)\n",
    "def obj_lin_fit_off(loss_func, amt_uM, rsp_ratio, p):\n",
    "    alpha = p[0]\n",
    "    beta = p[1]\n",
    "    y = lin_fit_off(rsp_ratio, alpha, beta)\n",
    "    loss = sum(loss_func(amt_uM, y))\n",
    "    return(loss)\n",
    "\n",
    "# Second degree polynomial fit intersecting 0:\n",
    "def pol2d_fit(x, beta, gamma):\n",
    "    return(x**2*beta + x*gamma)\n",
    "def obj_pol2d_fit(loss_func, amt_uM, rsp_ratio, p):\n",
    "    beta = p[0]\n",
    "    gamma = p[1]\n",
    "    y = pol2d_fit(rsp_ratio, beta, gamma)\n",
    "    loss = sum(loss_func(amt_uM, y))\n",
    "    return(loss)\n",
    "# Second degree polynomial fit allowed to be offset 0:\n",
    "def pol2d_fit_off(x, alpha, beta, gamma):\n",
    "    return(alpha + x**2*beta + x*gamma)\n",
    "def obj_pol2d_fit_off(loss_func, amt_uM, rsp_ratio, p):\n",
    "    alpha = p[0]\n",
    "    beta = p[1]\n",
    "    gamma = p[2]\n",
    "    y = pol2d_fit_off(rsp_ratio, alpha, beta, gamma)\n",
    "    loss = sum(loss_func(amt_uM, y))\n",
    "    return(loss)\n",
    "\n",
    "# Power fit intersecting 0:\n",
    "def pow_fit(x, beta, gamma):\n",
    "    return(beta*x**gamma)\n",
    "def obj_pow_fit(loss_func, amt_uM, rsp_ratio, p):\n",
    "    beta = p[0]\n",
    "    gamma = p[1]\n",
    "    y = pow_fit(rsp_ratio, beta, gamma)\n",
    "    loss = sum(loss_func(amt_uM, y))\n",
    "    return(loss)\n",
    "# Power fit allowed to be offset 0:\n",
    "def pow_fit_off(x, alpha, beta, gamma):\n",
    "    return(alpha + beta*x**gamma)\n",
    "def obj_pow_fit_off(loss_func, amt_uM, rsp_ratio, p):\n",
    "    alpha = p[0]\n",
    "    beta = p[1]\n",
    "    gamma = p[2]\n",
    "    y = pow_fit_off(rsp_ratio, alpha, beta, gamma)\n",
    "    loss = sum(loss_func(amt_uM, y))\n",
    "    return(loss)\n",
    "\n",
    "# Functions to pick the best interpolation, intersecting 0:\n",
    "def pick_best_fun(t, x):\n",
    "    bnds = ((0, None),)\n",
    "    def fun_lin(p): return(obj_lin_fit(loss_func_c1, t, x, p))\n",
    "    p_lin = minimize(fun_lin, (1), method='L-BFGS-B', bounds=bnds)\n",
    "\n",
    "    bnds = ((None, None), (0, None))\n",
    "    def fun_pol2d(p): return(obj_pol2d_fit(loss_func_c1, t, x, p))\n",
    "    p_pol2d = minimize(fun_pol2d, (1, 1), method='L-BFGS-B', bounds=bnds)\n",
    "\n",
    "    bnds = ((0, None), (0, None))\n",
    "    def fun_pow(p): return(obj_pow_fit(loss_func_c1, t, x, p))\n",
    "    p_pow = minimize(fun_pow, (0.4, 1), method='L-BFGS-B', bounds=bnds)\n",
    "\n",
    "    loss_list = [p_lin.fun, p_pol2d.fun, p_pow.fun]\n",
    "    func_list = [lambda x: lin_fit(x, p_lin.x[0]), lambda x: pol2d_fit(x, p_pol2d.x[0], p_pol2d.x[1]), lambda x: pow_fit(x, p_pow.x[0], p_pow.x[1])]\n",
    "    min_idx = loss_list.index(min(loss_list))\n",
    "    best_fun = func_list[min_idx]\n",
    "    return(best_fun)\n",
    "def pick_best_fun_l1(t, x):\n",
    "    bnds = ((0, None),)\n",
    "    def fun_lin(p): return(obj_lin_fit(loss_func_l1, t, x, p))\n",
    "    p_lin = minimize(fun_lin, (1), method='L-BFGS-B', bounds=bnds)\n",
    "\n",
    "    bnds = ((None, None), (0, None))\n",
    "    def fun_pol2d(p): return(obj_pol2d_fit(loss_func_l1, t, x, p))\n",
    "    p_pol2d = minimize(fun_pol2d, (1, 1), method='L-BFGS-B', bounds=bnds)\n",
    "\n",
    "    bnds = ((0, None), (0, None))\n",
    "    def fun_pow(p): return(obj_pow_fit(loss_func_l1, t, x, p))\n",
    "    p_pow = minimize(fun_pow, (0.4, 1), method='L-BFGS-B', bounds=bnds)\n",
    "\n",
    "    loss_list = [p_lin.fun, p_pol2d.fun, p_pow.fun]\n",
    "    func_list = [lambda x: lin_fit(x, p_lin.x[0]), lambda x: pol2d_fit(x, p_pol2d.x[0], p_pol2d.x[1]), lambda x: pow_fit(x, p_pow.x[0], p_pow.x[1])]\n",
    "    min_idx = loss_list.index(min(loss_list))\n",
    "    best_fun = func_list[min_idx]\n",
    "    return(best_fun)\n",
    "def pick_best_fun_l2(t, x):\n",
    "    bnds = ((0, None),)\n",
    "    def fun_lin(p): return(obj_lin_fit(loss_func_l2, t, x, p))\n",
    "    p_lin = minimize(fun_lin, (1), method='L-BFGS-B', bounds=bnds)\n",
    "\n",
    "    bnds = ((None, None), (0, None))\n",
    "    def fun_pol2d(p): return(obj_pol2d_fit(loss_func_l2, t, x, p))\n",
    "    p_pol2d = minimize(fun_pol2d, (1, 1), method='L-BFGS-B', bounds=bnds)\n",
    "\n",
    "    bnds = ((0, None), (0, None))\n",
    "    def fun_pow(p): return(obj_pow_fit(loss_func_l2, t, x, p))\n",
    "    p_pow = minimize(fun_pow, (0.4, 1), method='L-BFGS-B', bounds=bnds)\n",
    "\n",
    "    loss_list = [p_lin.fun, p_pol2d.fun, p_pow.fun]\n",
    "    func_list = [lambda x: lin_fit(x, p_lin.x[0]), lambda x: pol2d_fit(x, p_pol2d.x[0], p_pol2d.x[1]), lambda x: pow_fit(x, p_pow.x[0], p_pow.x[1])]\n",
    "    min_idx = loss_list.index(min(loss_list))\n",
    "    best_fun = func_list[min_idx]\n",
    "    return(best_fun)\n",
    "\n",
    "# Functions to pick the best interpolation allowed to be offset 0:\n",
    "def pick_best_fun_off(t, x):\n",
    "    bnds = ((None, None), (0, None))\n",
    "    def fun_lin(p): return(obj_lin_fit_off(loss_func_c1, t, x, p))\n",
    "    p_lin1 = minimize(fun_lin, (0, 1), method='L-BFGS-B', bounds=bnds)\n",
    "\n",
    "    bnds = ((None, None), (0, None))\n",
    "    def fun_lin(p): return(obj_lin_fit_off(loss_func_c1, t, x, p))\n",
    "    p_lin2 = minimize(fun_lin, (0, 1), method='SLSQP', bounds=bnds)\n",
    "    \n",
    "    bnds = ((None, None), (None, None), (0, None))\n",
    "    def fun_pol2d(p): return(obj_pol2d_fit_off(loss_func_c1, t, x, p))\n",
    "    p_pol2d1 = minimize(fun_pol2d, (0, 1, 1), method='L-BFGS-B', bounds=bnds)\n",
    "\n",
    "    bnds = ((None, None), (None, None), (0, None))\n",
    "    def fun_pol2d(p): return(obj_pol2d_fit_off(loss_func_c1, t, x, p))\n",
    "    p_pol2d2 = minimize(fun_pol2d, (0, 1, 1), method='SLSQP', bounds=bnds)\n",
    "    \n",
    "    bnds = ((None, None), (0, None), (0, None))\n",
    "    def fun_pow(p): return(obj_pow_fit_off(loss_func_c1, t, x, p))\n",
    "    p_pow1 = minimize(fun_pow, (0, 0.4, 1), method='L-BFGS-B', bounds=bnds)\n",
    "\n",
    "    bnds = ((None, None), (0, None), (0, None))\n",
    "    def fun_pow(p): return(obj_pow_fit_off(loss_func_c1, t, x, p))\n",
    "    p_pow2 = minimize(fun_pow, (0, 0.4, 1), method='SLSQP', bounds=bnds)\n",
    "    \n",
    "    loss_list = [p_lin1.fun, p_lin2.fun, p_pol2d1.fun, p_pol2d2.fun, p_pow1.fun, p_pow2.fun]\n",
    "    func_list = [lambda x: lin_fit_off(x, p_lin1.x[0], p_lin1.x[1]), lambda x: lin_fit_off(x, p_lin2.x[0], p_lin2.x[1]), lambda x: pol2d_fit_off(x, p_pol2d1.x[0], p_pol2d1.x[1], p_pol2d1.x[2]), lambda x: pol2d_fit_off(x, p_pol2d2.x[0], p_pol2d2.x[1], p_pol2d2.x[2]), lambda x: pow_fit_off(x, p_pow1.x[0], p_pow1.x[1], p_pow1.x[2]), lambda x: pow_fit_off(x, p_pow2.x[0], p_pow2.x[1], p_pow2.x[2])]\n",
    "    min_idx = loss_list.index(min(loss_list))\n",
    "    best_fun = func_list[min_idx]\n",
    "    return(best_fun)\n",
    "def pick_best_fun_off_l1(t, x):\n",
    "    bnds = ((None, None), (0, None))\n",
    "    def fun_lin(p): return(obj_lin_fit_off(loss_func_l1, t, x, p))\n",
    "    p_lin = minimize(fun_lin, (0, 1), method='L-BFGS-B', bounds=bnds)\n",
    "\n",
    "    bnds = ((None, None), (None, None), (0, None))\n",
    "    def fun_pol2d(p): return(obj_pol2d_fit_off(loss_func_l1, t, x, p))\n",
    "    p_pol2d = minimize(fun_pol2d, (0, 1, 1), method='L-BFGS-B', bounds=bnds)\n",
    "\n",
    "    bnds = ((None, None), (0, None), (0, None))\n",
    "    def fun_pow(p): return(obj_pow_fit_off(loss_func_l1, t, x, p))\n",
    "    p_pow = minimize(fun_pow, (0, 0.4, 1), method='L-BFGS-B', bounds=bnds)\n",
    "\n",
    "    loss_list = [p_lin.fun, p_pol2d.fun, p_pow.fun]\n",
    "    func_list = [lambda x: lin_fit_off(x, p_lin.x[0], p_lin.x[1]), lambda x: pol2d_fit_off(x, p_pol2d.x[0], p_pol2d.x[1], p_pol2d.x[2]), lambda x: pow_fit_off(x, p_pow.x[0], p_pow.x[1], p_pow.x[2])]\n",
    "    min_idx = loss_list.index(min(loss_list))\n",
    "    best_fun = func_list[min_idx]\n",
    "    return(best_fun)\n",
    "\n",
    "# Function to pick the best extrapolation:\n",
    "def pick_lin_fun(t, x, off):\n",
    "    if off is True:\n",
    "        bnds = ((None, None), (0, None))\n",
    "        def fun_lin(p): return(obj_lin_fit_off(loss_func_l1, t, x, p))\n",
    "        p_lin = minimize(fun_lin, (0, 1), method='L-BFGS-B', bounds=bnds)\n",
    "        return(lambda x: lin_fit_off(x, p_lin.x[0], p_lin.x[1]))\n",
    "    elif off is False:\n",
    "        bnds = ((0, None),)\n",
    "        def fun_lin(p): return(obj_lin_fit(loss_func_l1, t, x, p))\n",
    "        p_lin = minimize(fun_lin, (1), method='L-BFGS-B', bounds=bnds)\n",
    "        return(lambda x: lin_fit(x, p_lin.x[0]))\n",
    "    else:\n",
    "        raise Exception('Unrecognized off: {}'.format(off))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_piece_wise_fit(amt_uM, rsp_ratio, amt_uM_range, rsp_ratio_range, df, debug=True):\n",
    "    '''\n",
    "    This function returns a piecewise function y = f(x)\n",
    "    x = response ratio\n",
    "    y = imputed sample concentration\n",
    "    '''\n",
    "    piece_wise_fit = dict()\n",
    "    smo_frac = 0.4 # linear smoothening, fraction +/- response ratio\n",
    "\n",
    "    ### Extrapolate above highest response ratio ###\n",
    "    ub = rsp_ratio_range[0]\n",
    "    lb = rsp_ratio_range[1] + smo_frac*rsp_ratio_range[1]\n",
    "    key_extra_up = (ub, lb)\n",
    "\n",
    "    mask = (amt_uM == amt_uM_range[1]) | (amt_uM == amt_uM_range[2]) | (amt_uM == amt_uM_range[3]) | (amt_uM == amt_uM_range[4]) | (amt_uM == amt_uM_range[5])\n",
    "    amt_uM_tmp = amt_uM[mask]\n",
    "    rsp_ratio_tmp = rsp_ratio[mask]\n",
    "\n",
    "    # Restrict the extrapolation to linear fit:\n",
    "    # piece_wise_fit[key_extra_up] = pick_best_fun(amt_uM_tmp, rsp_ratio_tmp)\n",
    "    piece_wise_fit[key_extra_up] = pick_lin_fun(amt_uM_tmp, rsp_ratio_tmp, True)\n",
    "\n",
    "    ### Interpolate between highest and lowest response ratio ###\n",
    "    ub = rsp_ratio_range[1] - smo_frac*rsp_ratio_range[1]\n",
    "    lb = rsp_ratio_range[len(amt_uM_range)-2] + smo_frac*rsp_ratio_range[len(amt_uM_range)-2]\n",
    "    key_intra = (ub, lb)\n",
    "    piece_wise_fit[key_intra] = pick_best_fun_off(amt_uM, rsp_ratio)\n",
    "\n",
    "    ### Smoothen between the upper extrapolation and the intrapolation ###\n",
    "    ub = rsp_ratio_range[1] + smo_frac*rsp_ratio_range[1]\n",
    "    lb = rsp_ratio_range[1] - smo_frac*rsp_ratio_range[1]\n",
    "    key_between_intra_up = (ub, lb)\n",
    "    \n",
    "    y_upper = piece_wise_fit[key_extra_up](ub)\n",
    "    y_lower = piece_wise_fit[key_intra](lb)\n",
    "\n",
    "    beta_upper = (y_upper - y_lower) / (ub - lb)\n",
    "    if beta_upper < 0:\n",
    "        print('Upper smoothening gave negative slope!!! Maybe adjust smo_frac.')\n",
    "    alpha_upper = y_upper - beta_upper*ub\n",
    "    piece_wise_fit[key_between_intra_up] = lambda x: alpha_upper + beta_upper*x\n",
    "\n",
    "    ### Extrapolate between 0 and lowest response ratio ###\n",
    "    ub = rsp_ratio_range[len(amt_uM_range)-2] - smo_frac*rsp_ratio_range[len(amt_uM_range)-2]\n",
    "    lb = rsp_ratio_range[len(amt_uM_range)-1]\n",
    "    key_extra_low = (ub, lb)\n",
    "\n",
    "    i = len(amt_uM_range)-2\n",
    "    # Extrapolate from the 5 lowest concentrations:\n",
    "    mask = (amt_uM == amt_uM_range[i-3]) | (amt_uM == amt_uM_range[i-2]) | (amt_uM == amt_uM_range[i-1]) | (amt_uM == amt_uM_range[i]) | (amt_uM == amt_uM_range[i+1])\n",
    "    amt_uM_tmp = amt_uM[mask]\n",
    "    rsp_ratio_tmp = rsp_ratio[mask]\n",
    "    piece_wise_fit[key_extra_low] = pick_best_fun_l1(amt_uM_tmp, rsp_ratio_tmp)\n",
    "    #piece_wise_fit[key_extra_low] = pick_lin_fun(amt_uM_tmp, rsp_ratio_tmp, False)\n",
    "\n",
    "    ### Smoothen between the lower extrapolation and the intrapolation ###\n",
    "    ub = rsp_ratio_range[len(amt_uM_range)-2] + smo_frac*rsp_ratio_range[len(amt_uM_range)-2]\n",
    "    lb = rsp_ratio_range[len(amt_uM_range)-2] - smo_frac*rsp_ratio_range[len(amt_uM_range)-2]\n",
    "    key_between_intra_low = (ub, lb)\n",
    "    \n",
    "    y_upper = piece_wise_fit[key_intra](ub)\n",
    "    y_lower = piece_wise_fit[key_extra_low](lb)\n",
    "\n",
    "    beta_lower = (y_upper - y_lower) / (ub - lb)\n",
    "    if beta_lower < 0:\n",
    "        print('Lower smoothening gave negative slope!!! Maybe adjust smo_frac.')\n",
    "    alpha_lower = y_upper - beta_lower*ub\n",
    "    piece_wise_fit[key_between_intra_low] = lambda x: alpha_lower + beta_lower*x\n",
    "\n",
    "    if debug is True:\n",
    "        plot_cal_curve(df, piece_wise_fit)\n",
    "\n",
    "    return(piece_wise_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_conc(piece_wise_fit_metab, response_ratio):\n",
    "    '''\n",
    "    This function imputes the concentration from a response ratio.\n",
    "    '''\n",
    "    response_ratio_range = np.array(list(piece_wise_fit_metab.keys()))\n",
    "    mask_range = [response_ratio >= min_v and response_ratio <= max_v for max_v, min_v in response_ratio_range]\n",
    "    k = tuple(response_ratio_range[mask_range][0])\n",
    "    return(piece_wise_fit_metab[k](response_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cal_curve(df, piece_wise_fit_metab):\n",
    "    '''\n",
    "    Plot for debugging.\n",
    "    '''\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    #plt.scatter(rr_mes_metab, imp_conc_mes_metab, color='red', marker='x')\n",
    "    ax = sns.scatterplot(x=\"Response Ratio\", y=\"Theoretical Amt\", data=df)\n",
    "    ax.set(xscale=\"log\", yscale=\"log\")\n",
    "    ax.set_title('{}'.format(metab))\n",
    "    for k in piece_wise_fit_metab.keys():\n",
    "        ub, lb = k\n",
    "        if ub == float('inf'):\n",
    "            ub = 3*lb\n",
    "        if lb == 0:\n",
    "            lb = ub * 1e-1\n",
    "        x = np.linspace(lb, ub, 1000)\n",
    "        y = piece_wise_fit_metab[k](x)\n",
    "        plt.plot(x, y, linewidth=1, color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cal_piece_wise_fit(metab, df, piece_wise_fit_metab, save=False, linscale=False):\n",
    "    '''\n",
    "    Plot the calibration curve data with the piece wise fit.\n",
    "    '''\n",
    "    if linscale: # Plot measured points on linear scale\n",
    "        plt.figure(figsize=(7, 6))\n",
    "        ax = sns.scatterplot(rr_mes_metab, imp_conc_mes_metab, color='red', marker='x')\n",
    "        ax.set_title('Lin. mes. {}'.format(metab))\n",
    "        plt.show()\n",
    "    else: # Plot both measured and calibration points on log-log scale\n",
    "        fig, ax1 = plt.subplots(1, 1, figsize=(6, 5.4))\n",
    "\n",
    "        g1 = sns.scatterplot(ax=ax1, x=\"Response Ratio\", y=\"Theoretical Amt\", data=df)\n",
    "        g1.set(xscale=\"log\", yscale=\"log\", ylabel='Concentration (Î¼M)')\n",
    "        g1.set_title('{}'.format(metab[0:-4]))\n",
    "        g1.grid(True)\n",
    "\n",
    "        for line_idx, k in enumerate(piece_wise_fit_metab.keys()):\n",
    "            ub, lb = k\n",
    "            if ub == float('inf'): # Extrapolation (over)\n",
    "                ub = 3*lb\n",
    "            if lb == 0: # Extrapolation (under)\n",
    "                lb = ub * 0.5\n",
    "            x = np.logspace(np.log10(lb), np.log10(ub), 1000)\n",
    "            y = piece_wise_fit_metab[k](x)\n",
    "            ax1.plot(x, y, linewidth=1, color='black', linestyle='--')\n",
    "            # ax1.plot(x, y, linewidth=2, color=palette[line_idx], linestyle='--')\n",
    "\n",
    "        fig.tight_layout()\n",
    "        if save:\n",
    "            return(fig)\n",
    "        else:\n",
    "            fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read calibration data:\n",
    "### Replace all N/F with 0 before start ###\n",
    "esheet_dict_cal = pd.read_excel('cal-curve_LCMS-data_runs/nucl_quant_april_22.xlsx', sheet_name=None)\n",
    "metab_dict_cal = dict()\n",
    "metab_names_cal = list()\n",
    "for k in esheet_dict_cal.keys():\n",
    "    if 'U-13C' not in k:\n",
    "        metab_names_cal.append(k)\n",
    "        # Remove non calibration standards and zero peaks:\n",
    "        df_tmp = copy.deepcopy(esheet_dict_cal[k])\n",
    "        df_tmp['Response Ratio'] = df_tmp['Area'].values / df_tmp['ISTD Response'].values\n",
    "        mask_cal = df_tmp['Theoretical Amt'].values > 0\n",
    "        metab_dict_cal[k] = df_tmp.loc[mask_cal]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-59ebc371b61c>:60: RuntimeWarning: overflow encountered in power\n",
      "  return(alpha + beta*x**gamma)\n",
      "/Users/krdav/anaconda3/lib/python3.8/site-packages/scipy/optimize/_numdiff.py:497: RuntimeWarning: invalid value encountered in subtract\n",
      "  df = fun(x) - f0\n",
      "<ipython-input-2-59ebc371b61c>:60: RuntimeWarning: overflow encountered in multiply\n",
      "  return(alpha + beta*x**gamma)\n",
      "<ipython-input-2-59ebc371b61c>:5: RuntimeWarning: overflow encountered in multiply\n",
      "  return((np.abs(y - t) / t) * 100)\n"
     ]
    }
   ],
   "source": [
    "### Fit calibration curve ###\n",
    "piece_wise_fit = dict()\n",
    "for metab in metab_names_cal:\n",
    "    amt_uM = metab_dict_cal[metab]['Theoretical Amt'].values\n",
    "    rsp_ratio = metab_dict_cal[metab]['Response Ratio'].values\n",
    "    df = metab_dict_cal[metab]\n",
    "    # Extract the fitting range:\n",
    "    amt_uM_range = sorted(np.unique(amt_uM), reverse=True)\n",
    "    rsp_ratio_range = [np.average(rsp_ratio[(amt_uM == r)]) for r in amt_uM_range]\n",
    "    amt_uM_range.append(0)\n",
    "    amt_uM_range.insert(0, float('inf'))\n",
    "    rsp_ratio_range.append(0)\n",
    "    rsp_ratio_range.insert(0, float('inf'))\n",
    "    # Fit the data:\n",
    "    piece_wise_fit[metab] = make_piece_wise_fit(amt_uM, rsp_ratio, amt_uM_range, rsp_ratio_range, df, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot measured values on calibration curve ###\n",
    "save_pdf = True\n",
    "if save_pdf:\n",
    "    pdf = matplotlib.backends.backend_pdf.PdfPages(\"calibration_curves_april_22.pdf\")\n",
    "    for metab in metab_names_cal:\n",
    "        fig = plot_cal_piece_wise_fit(metab, metab_dict_cal[metab], piece_wise_fit[metab], save=save_pdf)\n",
    "        pdf.savefig(fig)\n",
    "        plt.close(fig)\n",
    "    pdf.close()\n",
    "else:\n",
    "    for metab in metab_names_cal:\n",
    "        plot_cal_piece_wise_fit(metab, metab_dict_cal[metab], piece_wise_fit[metab], save=save_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle all the relevant compounds:\n",
    "dict_pickle_fnam = 'AA-nucleoside_quant-dict.pickle'\n",
    "all_comp = list(piece_wise_fit.keys())\n",
    "not_quant = ['Adenine pos', 'Cytosine pos', 'Guanine pos', 'Thymine neg', 'Uracil neg']\n",
    "for metab in not_quant:\n",
    "    if metab in piece_wise_fit:\n",
    "        del piece_wise_fit[metab]\n",
    "\n",
    "with open(dict_pickle_fnam, 'wb') as handle:\n",
    "    pickle.dump(piece_wise_fit, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_pickle_fnam = 'AA-nucleoside_quant-dict.pickle'\n",
    "with open(dict_pickle_fnam, 'rb') as handle:\n",
    "    piece_wise_fit = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read measurements\n",
    "\n",
    "### Replace all N/F with 0 before start ###\n",
    "esheet_dict_mes = pd.read_excel('Asp-uptake-flux.xlsx', sheet_name=None)\n",
    "\n",
    "metab_dict_mes = dict()\n",
    "metab_names_mes = list()\n",
    "for k in esheet_dict_mes.keys():\n",
    "    if 'U-13C' not in k:\n",
    "        metab_names_mes.append(k)\n",
    "        # Remove peaks with no internal standard:\n",
    "        df_tmp = copy.deepcopy(esheet_dict_mes[k])\n",
    "        mask_zero = df_tmp['ISTD Response'].values > 0\n",
    "        # Remove blank:\n",
    "        metab_dict_mes[k] = df_tmp.loc[mask_zero]\n",
    "        metab_dict_mes[k]['Sample_name'] = [fn.split('_')[-1] for fn in metab_dict_mes[k]['Filename']]\n",
    "        \n",
    "        # CAA was reconstituted in 400uL instead of 500uL\n",
    "        # CAA peak will then be 5/4 times what it should be\n",
    "        ## RR will then be 4/5 of what it should be\n",
    "        # Correction factor: 5/4\n",
    "        metab_dict_mes[k]['Response Ratio'] = metab_dict_mes[k]['Response Ratio'] * 5/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Impute concentration and add to metabolite dataframe ###\n",
    "rr_mes = dict()\n",
    "imp_conc_mes = dict()\n",
    "for metab in metab_names_mes:\n",
    "    metab_split = metab.split()\n",
    "    if len(metab_split) == 3:\n",
    "        metab_no_iso = ' '.join([metab_split[0], metab_split[-1]])\n",
    "    elif len(metab_split) == 2:\n",
    "        metab_no_iso = metab\n",
    "    else:\n",
    "        raise Exception('{} not recognized metabolite name format'.format(metab))\n",
    "    conc_list = list()\n",
    "    for rr in metab_dict_mes[metab]['Response Ratio'].values:\n",
    "        conc = impute_conc(piece_wise_fit[metab_no_iso], rr)\n",
    "        conc_list.append(conc)\n",
    "    metab_dict_mes[metab]['imputed_sample_conc'] = conc_list\n",
    "\n",
    "    if metab_no_iso in rr_mes:\n",
    "        rr_mes[metab_no_iso].extend(list(metab_dict_mes[metab]['Response Ratio'].values))\n",
    "        imp_conc_mes[metab_no_iso].extend(list(metab_dict_mes[metab]['imputed_sample_conc'].values))\n",
    "    else:\n",
    "        rr_mes[metab_no_iso] = list(metab_dict_mes[metab]['Response Ratio'].values)\n",
    "        imp_conc_mes[metab_no_iso] = list(metab_dict_mes[metab]['imputed_sample_conc'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dil_fact_df = pd.read_csv('Asp-uptake-flux_meta.csv', sep=',')\n",
    "dil_fact_dict = {k:v for k, v in zip(dil_fact_df['Sample_name'].values, dil_fact_df['dilution_factor'].values)}\n",
    "vol_hour_dict = {k:v for k, v in zip(dil_fact_df['Sample_name'].values, dil_fact_df['vol_hour'].values)}\n",
    "media_vol_dict = {k:v for k, v in zip(dil_fact_df['Sample_name'].values, dil_fact_df['Media_vol'].values)}\n",
    "time_dict = {k:v for k, v in zip(dil_fact_df['Sample_name'].values, dil_fact_df['Time'].values)}\n",
    "\n",
    "for metab in metab_names_mes:\n",
    "    dil_fact = [dil_fact_dict[sn] for sn in metab_dict_mes[metab]['Sample_name']]\n",
    "    vol_hour = [vol_hour_dict[sn] for sn in metab_dict_mes[metab]['Sample_name']]\n",
    "    media_vol = [media_vol_dict[sn] for sn in metab_dict_mes[metab]['Sample_name']]\n",
    "    time = [time_dict[sn] for sn in metab_dict_mes[metab]['Sample_name']]\n",
    "    metab_dict_mes[metab]['dilution_factor'] = dil_fact\n",
    "    metab_dict_mes[metab]['vol_hour'] = vol_hour\n",
    "    metab_dict_mes[metab]['Media_vol'] = media_vol\n",
    "    metab_dict_mes[metab]['Time'] = time\n",
    "    metab_dict_mes[metab]['undil_sample_conc'] = metab_dict_mes[metab]['imputed_sample_conc'] / metab_dict_mes[metab]['dilution_factor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add flux:\n",
    "for metab in metab_names_mes:\n",
    "    metab_dict_mes[metab]['fmoles'] = metab_dict_mes[metab]['undil_sample_conc'] * metab_dict_mes[metab]['Media_vol']\n",
    "    \n",
    "    blank_fmoles = metab_dict_mes[metab][metab_dict_mes[metab]['Time'] == 0]['fmoles'].mean()\n",
    "    metab_dict_mes[metab]['nmoles_depleted'] = (blank_fmoles - metab_dict_mes[metab]['fmoles']) * 1e-3\n",
    "    metab_dict_mes[metab]['Influx'] = metab_dict_mes[metab]['nmoles_depleted'] / metab_dict_mes[metab]['vol_hour'] * 1e-6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['Samples_name'] + metab_names_mes\n",
    "data0 = np.zeros((len(metab_dict_mes['Aspartate neg']), len(col_names)))\n",
    "flux_df = pd.DataFrame(data0, columns=col_names)\n",
    "\n",
    "flux_df['Samples_name'] = metab_dict_mes['Aspartate neg']['Sample_name'].values\n",
    "for metab in metab_names_mes:\n",
    "    flux_df[metab] = metab_dict_mes[metab]['Influx'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
